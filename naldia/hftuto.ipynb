{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c5233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de la bibliothèque transformers de Hugging Face\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8addf8",
   "metadata": {},
   "source": [
    "# Analyse des sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce283a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 953/953 [00:00<00:00, 955kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 669M/669M [00:58<00:00, 11.5MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 39.0/39.0 [00:00<00:00, 15.0kB/s]\n",
      "vocab.txt: 100%|██████████| 872k/872k [00:00<00:00, 7.51MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 100kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.5269569754600525}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Pipeline pour l'analyse de sentiments\n",
    "# analyse_sentiments = pipeline('sentiment-analysis', model=\"tblard/tf-allocine\")\n",
    "analyse_sentiments = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Exemple d'analyse de sentiment\n",
    "analyse_sentiments(\"J'aime beaucoup la soupe miso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0c477",
   "metadata": {},
   "source": [
    "# Traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d546ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.21k/1.21k [00:00<?, ?B/s]\n",
      "model.safetensors: 100%|██████████| 892M/892M [01:21<00:00, 11.0MB/s] \n",
      "generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 73.2kB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 3.06MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 3.25MB/s]\n",
      "c:\\Users\\gael.jaunin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'apprends au cours du défi #30daysgenerativeai.\n"
     ]
    }
   ],
   "source": [
    "# Pipeline pour la traduction (Anglais vers Français par exemple)\n",
    "traducteur = pipeline('translation_en_to_fr', model='t5-base')\n",
    "\n",
    "# Traduction d'une phrase de l'anglais vers le français\n",
    "phrase_traduite = traducteur(\"I am learning during the #30daysgenerativeai challenge.\")\n",
    "\n",
    "# Affichage de la phrase traduite\n",
    "print(phrase_traduite[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c5d30",
   "metadata": {},
   "source": [
    "# Generation de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ccef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le Challenge #30daysgenerativeai c'est pas pour tout de suite, mais je suis pas sûr que ça soit la bonne solution. Le premier est un peu plus long, mais il est très agréable à lire. Il est donc important de bien choisir son matériel. Il est donc important de bien choisir son matériel de ski. Il est donc important de bien choisir son matériel de ski. Il est donc important de bien choisir son matériel de ski. Il est donc important de bien choisir son matériel de ski. Le premier est un peu plus long, mais il est très agréable à lire. Il est donc important de bien choisir son matériel de ski. Le premier est un peu plus long, mais il est très agréable à lire. Il est donc important de bien choisir son matériel. Le premier est un peu plus long, mais il est très agréable à lire. Il est donc important de bien choisir son matériel. Le premier est un peu plus long, mais il est très agréable à lire\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Chargement du tokenizer et du modèle pour la génération de texte\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"antoiloui/belgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"antoiloui/belgpt2\")\n",
    "\n",
    "# Préparation des entrées pour la génération de texte, avec attention_mask\n",
    "inputs = tokenizer(\"Le Challenge #30daysgenerativeai c'est\", return_tensors=\"pt\")\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Génération de texte en français en fournissant l'attention_mask et en définissant pad_token_id si nécessaire\n",
    "text_generation = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id  # Définir si nécessaire\n",
    ")\n",
    "\n",
    "# Affichage du texte généré\n",
    "print(tokenizer.decode(text_generation[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee758fc7",
   "metadata": {},
   "source": [
    "# Classification zeto-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db167f",
   "metadata": {},
   "source": [
    "La classification à zero-shot fait référence à la capacité d'un modèle à classer correctement des textes dans des catégories sans avoir reçu d'exemples spécifiques lors de son entraînement. \n",
    "\n",
    "Le modèle utilise sa compréhension générale de la langue pour faire des hypothèses sur la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7451a00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"Cet article traite de l'importance de l'intelligence artificielle dans la société moderne.\",\n",
       " 'labels': ['technologie', 'politique', 'éducation', 'économie'],\n",
       " 'scores': [0.7246902585029602,\n",
       "  0.0945177674293518,\n",
       "  0.09433727711439133,\n",
       "  0.08645474910736084]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Pipeline pour la classification à zéro coup\n",
    "classify_zero_shot = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Exemple de classification à zéro coup en français\n",
    "classify_zero_shot(\n",
    "    \"Cet article traite de l'importance de l'intelligence artificielle dans la société moderne.\",\n",
    "    candidate_labels=[\"éducation\", \"politique\", \"technologie\", \"économie\"],\n",
    "    hypothesis_template=\"Cet article est sur {}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beddf3c7",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b1bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Je', 'suis', 'ra', '##vi', 'de', 'découvrir', 'des', 'tuto', '##rie', '##ls', 'sur', 'l', \"'\", 'intelligence', 'arti', '##ficie', '##lle', '.']\n",
      "Identifiants de Tokens: [101, 13796, 49301, 11859, 11310, 10104, 91134, 10139, 69635, 12904, 11747, 10326, 180, 112, 30151, 46118, 72138, 11270, 119, 102]\n",
      "Résultat de l'analyse de sentiment: [{'label': 'LABEL_1', 'score': 0.6690822243690491}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Nom du modèle pré-entraîné que vous souhaitez utiliser\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# Chargement du tokenizer et du modèle pour la classification de séquences\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Création d'une pipeline de classification de sentiment\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Exemple de texte à analyser\n",
    "texte = \"Je suis ravi de découvrir des tutoriels sur l'intelligence artificielle.\"\n",
    "\n",
    "# Tokenisation du texte\n",
    "tokens = tokenizer.tokenize(texte)\n",
    "input_ids = tokenizer.encode(texte)\n",
    "\n",
    "# Affichage des tokens et des identifiants de tokens\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Identifiants de Tokens:\", input_ids)\n",
    "\n",
    "# Exécution de la pipeline de classification de sentiment sur le texte\n",
    "resultat = classifier(texte)\n",
    "\n",
    "# Affichage du résultat de l'analyse de sentiment\n",
    "print(\"Résultat de l'analyse de sentiment:\", resultat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23de2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Je', 'suis', 'ra', '##vi', 'de', 'découvrir', 'des', 'tuto', '##rie', '##ls', 'sur', 'l', \"'\", 'intelligence', 'arti', '##ficie', '##lle', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Nom du modèle pré-entraîné que vous souhaitez utiliser\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# Chargement du tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Exemple de texte à tokeniser\n",
    "sequence = \"Je suis ravi de découvrir des tutoriels sur l'intelligence artificielle.\"\n",
    "\n",
    "# Tokenisation de la séquence\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08709a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifiants de Tokens: [13796, 49301, 11859, 11310, 10104, 91134, 10139, 69635, 12904, 11747, 10326, 180, 112, 30151, 46118, 72138, 11270, 119]\n"
     ]
    }
   ],
   "source": [
    "# Conversion des tokens en identifiants de tokens\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Identifiants de Tokens:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91da6715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Séquence décodée: Je suis ravi de découvrir des tutoriels sur l'intelligence artificielle.\n"
     ]
    }
   ],
   "source": [
    "# Décodage des identifiants de tokens pour récupérer la séquence\n",
    "decoded_sequence = tokenizer.decode(token_ids)\n",
    "print(\"Séquence décodée:\", decoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477f305",
   "metadata": {},
   "source": [
    "# Faire un résumé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b3eb4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.58k/1.58k [00:00<00:00, 794kB/s]\n",
      "model.safetensors: 100%|██████████| 1.63G/1.63G [02:21<00:00, 11.5MB/s]\n",
      "generation_config.json: 100%|██████████| 363/363 [00:00<00:00, 182kB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 2.26MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.55MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 7.10MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'intelligence artificielle est un domaine de l'informatique. Les algorithmes d'apprentissage automatique permettent aux ordinateurs de s'entraîner sur des donn\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Charger la pipeline de résumé avec le modèle BART\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Exemple de texte long à résumer\n",
    "text = \"\"\"\n",
    "L'intelligence artificielle est un domaine de l'informatique qui met l'accent sur la création de machines capables de travailler et de réagir comme des humains. Certains des exemples de travail dans ce domaine sont l'apprentissage automatique, où les ordinateurs, les logiciels et les appareils effectuent via des algorithmes des tâches de manière intelligente. Les algorithmes d'apprentissage automatique, qui sont au cœur de l'intelligence artificielle, permettent aux ordinateurs de s'entraîner sur des données fournies puis d'utiliser ces données pour prédire et prendre des décisions basées sur de nouvelles données. Les avantages de l'intelligence artificielle sont nombreux et peuvent avoir un impact significatif sur les secteurs où la précision et la cohérence sont cruciales.\n",
    "\"\"\"\n",
    "\n",
    "# Effectuer le résumé\n",
    "summary = summarizer(text, max_length=50, min_length=30, do_sample=False)\n",
    "\n",
    "# Afficher le résumé\n",
    "print(summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b43c4",
   "metadata": {},
   "source": [
    "# Générer une description de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a65398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "c:\\Users\\gael.jaunin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:101: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create an image segmentation pipeline\n",
    "from transformers import pipeline\n",
    "image_segmentation_pipeline = pipeline(\n",
    "    task=\"image-segmentation\",\n",
    "    model=\"nvidia/segformer-b5-finetuned-ade-640-640\"\n",
    ")\n",
    "\n",
    "# Path to your input image\n",
    "input_image_path = \"C:\\\\Users\\\\gael.jaunin\\\\OneDrive - Naldeo\\\\Documents\\\\1.NDC\\\\1-NALDIA\\\\v0\\\\auth\\\\chatapp\\\\sharepoint\\\\models\\\\castle.jpg\"\n",
    "\n",
    "\n",
    "# Perform image segmentation\n",
    "result = image_segmentation_pipeline(input_image_path)\n",
    "\n",
    "# Access the segmentation mask\n",
    "segmentation_mask = result[0]['score']\n",
    "print(segmentation_mask)\n",
    "# You can now use the segmentation mask as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac2c25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n",
    "def generer_legende_en_francais(chemin_image):\n",
    "    \"\"\"\n",
    "    Génère une légende pour une image donnée en anglais et la traduit en français.\n",
    "\n",
    "    :param chemin_image: Le chemin vers l'image pour laquelle générer une légende.\n",
    "    :type chemin_image: str\n",
    "    :return: La légende traduite en français.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialiser la pipeline de légendage d'image\n",
    "    pipeline_legende_image = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "    # Initialiser la pipeline de traduction de l'anglais vers le français\n",
    "    pipeline_traduction = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "    \n",
    "    # Génération de la légende en anglais\n",
    "    resultats_legende = pipeline_legende_image(chemin_image)\n",
    "    legende_anglaise = resultats_legende[0]['generated_text']\n",
    "\n",
    "    # Traduction de la légende en français\n",
    "    legende_francaise = pipeline_traduction(legende_anglaise, max_length=512)\n",
    "    texte_francais = legende_francaise[0]['translation_text']\n",
    "\n",
    "    return texte_francais\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7958391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.42k/1.42k [00:00<00:00, 472kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 301M/301M [00:26<00:00, 11.6MB/s] \n",
      "generation_config.json: 100%|██████████| 293/293 [00:00<00:00, 234kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 42.0/42.0 [00:00<?, ?B/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Exemple d'appel de la fonction\u001b[39;00m\n\u001b[0;32m      3\u001b[0m joblib_backups \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mgael.jaunin\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive - Naldeo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m1.NDC\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m1-NALDIA\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mv0\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mchatapp\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msharepoint\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m legende_francaise \u001b[38;5;241m=\u001b[39m \u001b[43mgenerer_legende_en_francais\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoblib_backups\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphoto.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(legende_francaise)\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36mgenerer_legende_en_francais\u001b[1;34m(chemin_image)\u001b[0m\n\u001b[0;32m     16\u001b[0m pipeline_legende_image \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage-to-text\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/blip-image-captioning-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Initialiser la pipeline de traduction de l'anglais vers le français\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pipeline_traduction \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHelsinki-NLP/opus-mt-en-fr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Génération de la légende en anglais\u001b[39;00m\n\u001b[0;32m     22\u001b[0m resultats_legende \u001b[38;5;241m=\u001b[39m pipeline_legende_image(chemin_image)\n",
      "File \u001b[1;32mc:\\Users\\gael.jaunin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:967\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    964\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    965\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 967\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    968\u001b[0m             tokenizer_identifier, use_fast\u001b[38;5;241m=\u001b[39muse_fast, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs\n\u001b[0;32m    969\u001b[0m         )\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gael.jaunin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:810\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    811\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    813\u001b[0m             )\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    818\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "# Exemple d'appel de la fonction\n",
    "\n",
    "joblib_backups = \"C:\\\\Users\\\\gael.jaunin\\\\OneDrive - Naldeo\\\\Documents\\\\1.NDC\\\\1-NALDIA\\\\v0\\\\auth\\\\chatapp\\\\sharepoint\\\\models\\\\\"\n",
    "legende_francaise = generer_legende_en_francais(joblib_backups+\"photo.jpg\")\n",
    "print(legende_francaise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d0790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
